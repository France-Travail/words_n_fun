{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook introducing the words_n_fun module\n",
    "# Copyright (C) <2018-2022>  <Agence Data Services, DSI PÃ´le Emploi>\n",
    "# \n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero General Public License as\n",
    "# published by the Free Software Foundation, either version 3 of the\n",
    "# License, or (at your option) any later version.\n",
    "# \n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "# \n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial notebook for the words_n_fun module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook highlights how to use preprocessing features of the words_n_fun module on a given text corpus. \n",
    "\n",
    "To do so, we will work on an English dataset, `comments.csv`, which is located alongside this notebook.  \n",
    "This dataset contains **several thousands comments about youtube videos**, from https://www.kaggle.com/datasets/advaypatil/youtube-statistics.\n",
    "\n",
    "The package structure (as of 09/2022) looks like this :\n",
    "```\n",
    ".\n",
    "â”œâ”€â”€ configs\n",
    "â”‚Â Â  â””â”€â”€ pipeline_usage_order.json\n",
    "â”œâ”€â”€ __init__.py\n",
    "â”œâ”€â”€ nltk_data\n",
    "â”‚Â Â  â””â”€â”€ corpora\n",
    "â”‚Â Â      â””â”€â”€ stopwords\n",
    "â”‚Â Â          â””â”€â”€ french\n",
    "â”œâ”€â”€ preprocessing\n",
    "â”‚Â Â  â”œâ”€â”€ api.py\n",
    "â”‚Â Â  â”œâ”€â”€ basic.py\n",
    "â”‚Â Â  â”œâ”€â”€ __init__.py\n",
    "â”‚Â Â  â”œâ”€â”€ lemmatizer.py\n",
    "â”‚Â Â  â”œâ”€â”€ split_sentences.py\n",
    "â”‚Â Â  â”œâ”€â”€ stopwords.py\n",
    "â”‚Â Â  â”œâ”€â”€ synonym_malefemale_replacement.py\n",
    "â”‚Â Â  â””â”€â”€ vectorization_tokenization.py\n",
    "â””â”€â”€ utils.py\n",
    "```\n",
    "\n",
    "The `utils.py` file provides utilities functions. The `configs` subfolder includes a json file that will be use to trigger warnings if preprocess functions are called in the wrong order. The `nltk_data` provides data to be used with nltk (by now only in French).\n",
    "\n",
    "The most important part is the `preprocessing` subfolder :\n",
    "\n",
    "- `basic.py` : this file exposes **all available preprocessing functions**. These functions preprocess pandas Series, but we added a decoraror `utils.data_agnostic` that makes it available to process strings, list of strings, np.arrays and pandas DataFrame (it uses either a `prefered_column` arg or the first column).\n",
    "- **`api.py`** : This file includes **the main entry point : `preprocess_pipeline`**. This functions takes input data and apply a preprocessing pipeline to it. It also manages different input types (same types as `utils.data_agnostic`).\n",
    "- `lemmatizer.py`, `split_sentences.py`, `stopwords.py`, `synonym_malefemale_replacement.py`, `vectorization_tokenization.py` : these files contain more complex and specific preprocessing functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from words_n_fun import utils\n",
    "from words_n_fun.preprocessing import api, basic\n",
    "\n",
    "# Reduce amount of logs for wnf\n",
    "import logging\n",
    "logging.getLogger('words_n_fun').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "\n",
    "Here we load the dataset into a pandas dataframe, and we then extract the pandas series to be preprocessed (i.e. the `Comment` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage path & load as a pd dataframe\n",
    "dir_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "file_path = os.path.join(dir_path, \"comments.csv\")\n",
    "df = pd.read_csv(file_path, sep=',', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the first 3 rows of the dataset\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset :\n",
    "print(f\"The loaded dataset has {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The preprocessing will be applied to the \"Comment\" column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select pd Series to be preprocessed\n",
    "docs = df[\"Comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said in the introduction, the main entry point for words_n_fun preprocessing is the `api.preprocess_pipeline`. This function takes tow main arguments :\n",
    "- `docs` : the data to be preprocessed (str, list, np.ndarray, pd.Series or pd.DataFrame)\n",
    "- `pipeline` : a list of preprocessing functions to successively apply to the input data. Some basic functions are listed in the `api.USAGE` dictionnary. Hence, we can use string keys instead of functions in the pipeline definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by using a simple preprocessing pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_1 = ['remove_non_string', 'to_lower', 'remove_punct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline will :\n",
    "- replace all NaNs by en empty string\n",
    "- convert all letters to lowercase\n",
    "- remove (most of) the ponctuation\n",
    "\n",
    "Let's try it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, on a string\n",
    "print('\\n---------------\\n')\n",
    "test_str = 'This is a test !'\n",
    "print(f\"{test_str} ---> {api.preprocess_pipeline(test_str, pipeline_1)}\")\n",
    "# Then on a list\n",
    "print('\\n---------------\\n')\n",
    "test_list = ['This is a test !', 'Btw, this sentence is also a test ;)']\n",
    "print(f\"{test_list} ---> {api.preprocess_pipeline(test_list, pipeline_1)}\")\n",
    "# Then on an np array\n",
    "print('\\n---------------\\n')\n",
    "test_np_array = np.array(['This is a test !', 'Btw, this sentence is also a test ;)'])\n",
    "print(f\"{test_np_array} ---> {api.preprocess_pipeline(test_np_array, pipeline_1)}\")\n",
    "# Then on a pd Series\n",
    "print('\\n---------------\\n')\n",
    "test_pd_series = pd.Series(['This is a test !', 'Btw, this sentence is also a test ;)'])\n",
    "print(f\"{test_pd_series} \\n--->\\n {api.preprocess_pipeline(test_pd_series, pipeline_1)}\")\n",
    "# Then on a DataFrame\n",
    "print('\\n---------------\\n')\n",
    "test_pd_dataframe = pd.DataFrame({'col1' : ['Test 1.', 'Test 2!'], 'col2' : ['Test 3?', 'Test 4$']})\n",
    "print(f\"{test_pd_dataframe} \\n--->\\n {api.preprocess_pipeline(test_pd_dataframe, pipeline_1)}\")\n",
    "# Finally on a DataFrame - version 2\n",
    "print('\\n---------------\\n')\n",
    "test_pd_dataframe = pd.DataFrame({'col1' : ['Test 1.', 'Test 2!'], 'col2' : ['Test 3?', 'Test 4$']})\n",
    "print(f\"{test_pd_dataframe} \\n--->\\n {api.preprocess_pipeline(test_pd_dataframe, pipeline_1, prefered_column='col2', modify_data=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this function can process many type of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not stuck with the provided functions only ! You can use custom functions ðŸ˜Š Let's try it ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function replaces all 'mr.' with 'mister'. We provide a function `get_regex_match_words` to\n",
    "# automatically create the correct regex. You can have a look at it, but we could have use any other preprocessing function.\n",
    "def my_custom_function(docs: pd.Series):\n",
    "    '''Replaces 'mr.' with 'mister' '''\n",
    "    my_regex = utils.get_regex_match_words(['mr.'], case_insensitive=True, words_as_regex=False)\n",
    "    docs = docs.str.replace(my_regex, 'mister', regex=True)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it alone\n",
    "my_custom_function(pd.Series(['Hello Mr. Smith']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use it in a pipeline !\n",
    "# We add utils.data_agnostic to make it work with more input types\n",
    "pipeline_2 = ['remove_non_string', 'to_lower', utils.data_agnostic(my_custom_function), 'remove_punct']\n",
    "test_str = 'Hello Mr. Smith !'\n",
    "print(f\"{test_str} ---> {api.preprocess_pipeline(test_str, pipeline_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt existing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before, we can use custom functions in our pipelines. But we can also adapt existing functions by modifying the default kwargs. To do so, we will use partial functions (`functools.partial`). \n",
    "\n",
    "Let's try it with `remove_stopwords`. We want to remove `hello` and `test` from our texts. To do so, we will reuse the `remove_stopwords` function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_remove_stopwords = functools.partial(basic.remove_stopwords, opt='none', set_to_add=['hello', 'test', 'Hello', 'Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it alone\n",
    "new_remove_stopwords(pd.Series(['Hello, this is a test !']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use it in a pipeline !\n",
    "# We add utils.data_agnostic to make it work with more input types\n",
    "pipeline_3 = ['remove_non_string', 'to_lower', new_remove_stopwords, 'remove_punct']\n",
    "test_str = 'Hello, this is a test !'\n",
    "print(f\"{test_str} ---> {api.preprocess_pipeline(test_str, pipeline_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works like a charm !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full example on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : create a full pipeline, showcase each step, use it on the whole dataset, process by chunck, use it directly on the .csv ? (not advise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing on the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample :\n",
    "docs=df[\"description\"][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here we define the desired pipeline.</p>\n",
    "<p>Transformations are applied in the same order in which they are specified :</p>\n",
    "<ul>\n",
    "    <li>**remove_non_string** : Removes non string characters</li>  \n",
    "    <li>**get_true_spaces** : Replaces all white spaces with a single space</li>\n",
    "        <li>**to_lower_except_singleletters** : Lower case transformation except for single letters (such as language R or language C)</li>\n",
    "        \n",
    "    <li>**pe_matching** : Basic one to one substitution \n",
    "        *Example* : \"permis b\" (french driving licence) => \"permisb\"</li>\n",
    "    <li>**remove_gender_synonyms** : Finds occurences where both male and female versions of a single words are used (eg: Serveur/Serveuse) and keep only the male version (language convention)</li>\n",
    "        \n",
    "    <li>**remove_punct_except_parenthesis** :  Removes all non alphanumeric characters by whitespaces except for parenthesis</li>\n",
    "    <li>**remove_numeric** : Returns a text without any numerical character</li>\n",
    "    <li>**remove_stopwords** : Returns a text without stopwords</li>\n",
    "    <li>**lemmatize** OU **stemmatize** : Text lemmatization or stemmatization\n",
    "    <li>**remove_accents** : Returns a text without any accent</li>\n",
    "    <li>**trim_string** : Replaces multiple white spaces by a single one</li>\n",
    "    <li>**remove_leading_and_ending_spaces** : Removes leadining and trailing white spaces</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline definition :\n",
    "pipeline = ['remove_non_string', 'get_true_spaces', 'to_lower_except_singleletters', 'pe_matching',\n",
    "                    'remove_gender_synonyms', 'remove_punct_except_parenthesis', 'remove_numeric',\n",
    "                    'remove_stopwords', 'stemmatize', 'remove_accents', 'trim_string', 'remove_leading_and_ending_spaces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the pipeline\n",
    "docs_preprocess = preprocessing.preprocess_pipeline(docs,\n",
    "                                                        pipeline=pipeline)\n",
    "docs_preprocess.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the first rows :\n",
    "for i in range(0,4) :\n",
    "    print(\"Document index nÂ°\",i,\"before preprocessing :\")\n",
    "    print(\"'\",docs[i],\"'\")\n",
    "    print(\"  and after preprocessing \")\n",
    "    print(\"'\",docs_preprocess[i],\"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Diving into each single step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only consider the first row of our initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=docs[0]\n",
    "text=pd.Series(text)\n",
    "print(text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ['notnull', 'remove_non_string', 'to_lower_except_singleletters', 'pe_matching', 'trim_string',\n",
    "                                        'remove_gender_synonyms', 'remove_punct_except_parenthesis', 'remove_numeric',\n",
    "                                        'remove_stopwords','lemmatize', 'remove_accents']\n",
    "def preprocess_pipeline_detail(text, pipeline=pipeline):\n",
    "    print (\"Texte initial\")\n",
    "    print (text.values)\n",
    "    for item in pipeline:\n",
    "        if item in api.USAGE.keys():\n",
    "            print(\"\\n\")\n",
    "            print(str(item))\n",
    "            text=api.USAGE[item](text)\n",
    "            print (text.values)\n",
    "            #print(\"Etape %s\" % item)\n",
    "            #print(list(text.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline_detail(text,pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wnf",
   "language": "python",
   "name": "venv_wnf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
