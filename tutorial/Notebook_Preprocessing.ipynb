{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook introducing the words_n_fun module\n",
    "# Copyright (C) <2018-2022>  <Agence Data Services, DSI PÃ´le Emploi>\n",
    "# \n",
    "# This program is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Affero General Public License as\n",
    "# published by the Free Software Foundation, either version 3 of the\n",
    "# License, or (at your option) any later version.\n",
    "# \n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "# \n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial notebook for the words_n_fun module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook highlights how to use the preprocessing features of the words_n_fun module on a given text corpus. \n",
    "\n",
    "To do so, we will work on an English dataset, `comments.csv`, which is located alongside this notebook.  \n",
    "This dataset contains **several thousands comments about youtube videos**, from https://www.kaggle.com/datasets/advaypatil/youtube-statistics.\n",
    "\n",
    "The package structure (as of 09/2022) looks like this :\n",
    "```\n",
    ".\n",
    "â”œâ”€â”€ configs\n",
    "â”‚Â Â  â””â”€â”€ pipeline_usage_order.json\n",
    "â”œâ”€â”€ __init__.py\n",
    "â”œâ”€â”€ nltk_data\n",
    "â”‚Â Â  â””â”€â”€ corpora\n",
    "â”‚Â Â      â””â”€â”€ stopwords\n",
    "â”‚Â Â          â””â”€â”€ french\n",
    "â”œâ”€â”€ preprocessing\n",
    "â”‚Â Â  â”œâ”€â”€ api.py\n",
    "â”‚Â Â  â”œâ”€â”€ basic.py\n",
    "â”‚Â Â  â”œâ”€â”€ __init__.py\n",
    "â”‚Â Â  â”œâ”€â”€ lemmatizer.py\n",
    "â”‚Â Â  â”œâ”€â”€ split_sentences.py\n",
    "â”‚Â Â  â”œâ”€â”€ stopwords.py\n",
    "â”‚Â Â  â”œâ”€â”€ synonym_malefemale_replacement.py\n",
    "â”‚Â Â  â””â”€â”€ vectorization_tokenization.py\n",
    "â””â”€â”€ utils.py\n",
    "```\n",
    "\n",
    "The `utils.py` file provides utilities functions. The `configs` subfolder includes a json file that will be use to trigger warnings if preprocess functions are called in the wrong order. The `nltk_data` provides data to be used with nltk (by now, only in French).\n",
    "\n",
    "The most important part is the `preprocessing` subfolder :\n",
    "\n",
    "- `basic.py` : this file exposes **all available preprocessing functions**. These functions preprocess pandas Series, but we added a decoraror `utils.data_agnostic` that makes it available to process strings, list of strings, np.arrays and pandas DataFrame (it uses either a `prefered_column` arg or the first column).\n",
    "- **`api.py`** : This file includes **the main entry point : `preprocess_pipeline`**. This functions takes input data and apply a preprocessing pipeline to it. It also manages different input types (same types as `utils.data_agnostic`).\n",
    "- `lemmatizer.py`, `split_sentences.py`, `stopwords.py`, `synonym_malefemale_replacement.py`, `vectorization_tokenization.py` : these files contain more complex and specific preprocessing functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from words_n_fun import utils\n",
    "from words_n_fun.preprocessing import api, basic\n",
    "\n",
    "# Reduce amount of logs for wnf\n",
    "import logging\n",
    "logging.getLogger('words_n_fun').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "\n",
    "Here we load the dataset into a pandas dataframe, and we then extract the pandas series to be preprocessed (i.e. the `Comment` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage path & load as a pd dataframe\n",
    "dir_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "file_path = os.path.join(dir_path, \"comments.csv\")\n",
    "df = pd.read_csv(file_path, sep=',', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the first 3 rows of the dataset\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset :\n",
    "print(f\"The loaded dataset has {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The preprocessing will be applied to the \"Comment\" column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select pd Series to be preprocessed\n",
    "docs = df[\"Comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said in the introduction, the main entry point for words_n_fun preprocessing is the `api.preprocess_pipeline`. This function takes two main arguments :\n",
    "- `docs` : the data to be preprocessed (str, list, np.ndarray, pd.Series or pd.DataFrame)\n",
    "- `pipeline` : a list of preprocessing functions to successively apply to the input data. Some basic functions are listed in the `api.USAGE` dictionnary. Hence, we can use string keys instead of functions in the pipeline definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by using a simple preprocessing pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_1 = ['remove_non_string', 'to_lower', 'remove_punct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline will :\n",
    "- replace all NaNs by en empty string\n",
    "- convert all letters to lowercase\n",
    "- remove (most of) the ponctuation\n",
    "\n",
    "Let's try it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, on a string\n",
    "print('\\n---------------\\n')\n",
    "test_str = 'This is a test !'\n",
    "print(f\"{test_str} ---> {api.preprocess_pipeline(test_str, pipeline_1)}\")\n",
    "# Then on a list\n",
    "print('\\n---------------\\n')\n",
    "test_list = ['This is a test !', 'Btw, this sentence is also a test ;)', None]\n",
    "print(f\"{test_list} ---> {api.preprocess_pipeline(test_list, pipeline_1)}\")\n",
    "# Then on an np array\n",
    "print('\\n---------------\\n')\n",
    "test_np_array = np.array(['This is a test !', 'Btw, this sentence is also a test ;)', None])\n",
    "print(f\"{test_np_array} ---> {api.preprocess_pipeline(test_np_array, pipeline_1)}\")\n",
    "# Then on a pd Series\n",
    "print('\\n---------------\\n')\n",
    "test_pd_series = pd.Series(['This is a test !', 'Btw, this sentence is also a test ;)', None])\n",
    "print(f\"{test_pd_series} \\n--->\\n {api.preprocess_pipeline(test_pd_series, pipeline_1)}\")\n",
    "# Then on a DataFrame\n",
    "print('\\n---------------\\n')\n",
    "test_pd_dataframe = pd.DataFrame({'col1' : ['Test 1.', 'Test 2!'], 'col2' : ['Test 3?', 'Test 4$']})\n",
    "print(f\"{test_pd_dataframe} \\n--->\\n {api.preprocess_pipeline(test_pd_dataframe, pipeline_1)}\")\n",
    "# Finally on a DataFrame - version 2\n",
    "print('\\n---------------\\n')\n",
    "test_pd_dataframe = pd.DataFrame({'col1' : ['Test 1.', 'Test 2!'], 'col2' : ['Test 3?', 'Test 4$']})\n",
    "print(f\"{test_pd_dataframe} \\n--->\\n {api.preprocess_pipeline(test_pd_dataframe, pipeline_1, prefered_column='col2', modify_data=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this function can process many type of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not stuck with the provided functions only ! You can use your custom functions ðŸ˜Š Let's try it ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function replaces all 'mr.' with 'mister'. We provide a function `get_regex_match_words` to\n",
    "# automatically create the correct regex. You can have a look at it, but we could have use any other preprocessing function.\n",
    "def my_custom_function(docs: pd.Series) -> pd.Series:\n",
    "    '''Replaces 'mr.' with 'mister' '''\n",
    "    my_regex = utils.get_regex_match_words(['mr.'], case_insensitive=True, words_as_regex=False)\n",
    "    docs = docs.str.replace(my_regex, 'mister', regex=True)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it alone\n",
    "my_custom_function(pd.Series(['Hello Mr. Smith']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use it in a pipeline !\n",
    "# We add utils.data_agnostic to make it work with more input types\n",
    "pipeline_2 = ['remove_non_string', 'to_lower', utils.data_agnostic(my_custom_function), 'remove_punct']\n",
    "test_str = 'Hello Mr. Smith !'\n",
    "print(f\"{test_str} ---> {api.preprocess_pipeline(test_str, pipeline_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt existing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen before, we can use custom functions in our pipelines. But we can also adapt existing functions by modifying the default kwargs. To do so, we will use partial functions (`functools.partial`). \n",
    "\n",
    "Let's try it with `remove_stopwords`. We want to remove `hello` and `test` from our texts. To do so, we will reuse the `remove_stopwords` function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_remove_stopwords = functools.partial(basic.remove_stopwords, opt='none', set_to_add=['hello', 'test', 'Hello', 'Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it alone\n",
    "new_remove_stopwords(pd.Series(['Hello, this is a test !']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use it in a pipeline !\n",
    "# We add utils.data_agnostic to make it work with more input types\n",
    "pipeline_3 = ['remove_non_string', 'to_lower', new_remove_stopwords, 'remove_punct']\n",
    "test_str = 'Hello, this is a test !'\n",
    "print(f\"{test_str} ---> {api.preprocess_pipeline(test_str, pipeline_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works like a charm !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full example on our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to create and use a preprocessing pipeline, we will now preprocess our dataset.  \n",
    "\n",
    "Let's first define our pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_4 = ['remove_non_string', 'get_true_spaces', 'remove_punct', 'to_lower', 'remove_numeric',\n",
    "              'trim_string', 'remove_leading_and_ending_spaces']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can show the result of each step :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to showcase each step results. Works only for predefined functions in api.USAGE.\n",
    "def preprocess_pipeline_detail(text: str, pipeline: list):\n",
    "    print(f\"\\033[94mInitial text :\\033[0m {text}\\n\")\n",
    "    for i, item in enumerate(pipeline):\n",
    "        text = api.USAGE[item](text)\n",
    "        print(f\"\\033[94mAfter preprocess {str(item)}:\\033[0m {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline_detail(\"      This is a test,    \\n\\n  with some  \\t puncTuAtIONs and numbers 1 2 3 4 !\", pipeline_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our preprocessing on the **whole dataset** !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "preprocessed_docs = api.preprocess_pipeline(docs, pipeline_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some results\n",
    "preprocessed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify a **chunck size** if the dataset is toohuge to be preprocessed at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's reactivate the logs just to showcase the chunck split\n",
    "logging.getLogger('words_n_fun').setLevel(logging.INFO)\n",
    "preprocessed_docs_2 = api.preprocess_pipeline(docs, pipeline_4, chunksize=5000)\n",
    "logging.getLogger('words_n_fun').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**/!\\ Not advised /!\\**\n",
    "\n",
    "We can even directly preprocessed the dataset by only providing it's file path.  This is not advised to do so, as this could be hazardous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess our dataset by providing it's path and the column to be preprocessed\n",
    "new_csv_path = api.preprocess_pipeline(file_path, pipeline_4, prefered_column='Comment', modify_data=False)\n",
    "print(new_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload it and check first rows\n",
    "df_preprocessed = pd.read_csv(new_csv_path, sep=',', encoding='utf-8', index_col=0)\n",
    "df_preprocessed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have covered the main principles of the words_n_fun library.  If you have any comments about this notebook, feel free to create an issue on github."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
